{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensorflow.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrUleTeclOEB"
      },
      "source": [
        "# Tensorflow Developer Certification 준비\r\n",
        "## 텐서플로우 자격증 준비를 위한 공부를 하려고 합니다.\r\n",
        "\r\n",
        "시험 문제 유형은 아래와 같으며 coursera와 udacity 강의를 보고 준비하려고 합니다.\r\n",
        "\r\n",
        "Category 1: Basic / Simple model\r\n",
        "\r\n",
        "Category 2: Model from learning dataset\r\n",
        "\r\n",
        "Category 3: Convolutional Neural Network with real-world image dataset\r\n",
        "\r\n",
        "Category 4: NLP Text Classification with real-world text dataset\r\n",
        "\r\n",
        "Category 5: Sequence Model with real-world numeric dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiGNWgRclT_r"
      },
      "source": [
        "# Category 1: Basic / Simple model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tPbHhrxlSjH"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import logging\r\n",
        "logger = tf.get_logger()\r\n",
        "logger.setLevel(logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-RkndVIljVU"
      },
      "source": [
        "celsius_q    = np.array([-40, -10,  0,  8, 15, 22,  38],  dtype=float)\r\n",
        "fahrenheit_a = np.array([-40,  14, 32, 46, 59, 72, 100],  dtype=float)\r\n",
        "\r\n",
        "for i,c in enumerate(celsius_q):\r\n",
        "  print(\"{} 섭씨는 = {} 화씨입니다.\".format(c, fahrenheit_a[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djKsKMqClnTk"
      },
      "source": [
        "l0 = tf.keras.layers.Dense(units=1, input_shape=[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0OPRh4YmZcI"
      },
      "source": [
        "model = tf.keras.Sequential([l0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtZ8YE2LmaQ_"
      },
      "source": [
        "model = tf.keras.Sequential([\r\n",
        "  tf.keras.layers.Dense(units=1, input_shape=[1])\r\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f_tksa6mbVv"
      },
      "source": [
        "model.compile(loss='mean_squared_error',\r\n",
        "              optimizer=tf.keras.optimizers.Adam(0.1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybboYtXqmdvg"
      },
      "source": [
        "history = model.fit(celsius_q, fahrenheit_a, epochs=500, verbose=False)\r\n",
        "print(\"Finished training the model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0mmDGn4mi3n"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "plt.xlabel('Epoch Number')\r\n",
        "plt.ylabel(\"Loss Magnitude\")\r\n",
        "plt.plot(history.history['loss'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBEsGLAPmgXs"
      },
      "source": [
        "print(model.predict([100.0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEHU-OzuoGcP"
      },
      "source": [
        "#Category 2: Model from learning dataset\r\n",
        "Fashion MNIST 데이터를 이용해서 간단한 모델을 만들고 학습을 시키려고 합니다.\r\n",
        "<table>\r\n",
        "  <tr><td>\r\n",
        "    <img src=\"https://tensorflow.org/images/fashion-mnist-sprite.png\"\r\n",
        "         alt=\"Fashion MNIST sprite\" width=\"600\">\r\n",
        "  </td></tr>\r\n",
        "  <tr><td align=\"center\">\r\n",
        "    <b>Figure 1.</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">Fashion-MNIST samples</a> (by Zalando, MIT License).<br/>&nbsp;\r\n",
        "  </td></tr>\r\n",
        "</table>\r\n",
        "\r\n",
        "<table>\r\n",
        "  <tr>\r\n",
        "    <th>Label</th>\r\n",
        "    <th>Class</th>\r\n",
        "  </tr>\r\n",
        "  <tr>\r\n",
        "    <td>0</td>\r\n",
        "    <td>T-shirt/top</td>\r\n",
        "  </tr>\r\n",
        "  <tr>\r\n",
        "    <td>1</td>\r\n",
        "    <td>Trouser</td>\r\n",
        "  </tr>\r\n",
        "    <tr>\r\n",
        "    <td>2</td>\r\n",
        "    <td>Pullover</td>\r\n",
        "  </tr>\r\n",
        "    <tr>\r\n",
        "    <td>3</td>\r\n",
        "    <td>Dress</td>\r\n",
        "  </tr>\r\n",
        "    <tr>\r\n",
        "    <td>4</td>\r\n",
        "    <td>Coat</td>\r\n",
        "  </tr>\r\n",
        "    <tr>\r\n",
        "    <td>5</td>\r\n",
        "    <td>Sandal</td>\r\n",
        "  </tr>\r\n",
        "    <tr>\r\n",
        "    <td>6</td>\r\n",
        "    <td>Shirt</td>\r\n",
        "  </tr>\r\n",
        "    <tr>\r\n",
        "    <td>7</td>\r\n",
        "    <td>Sneaker</td>\r\n",
        "  </tr>\r\n",
        "    <tr>\r\n",
        "    <td>8</td>\r\n",
        "    <td>Bag</td>\r\n",
        "  </tr>\r\n",
        "    <tr>\r\n",
        "    <td>9</td>\r\n",
        "    <td>Ankle boot</td>\r\n",
        "  </tr>\r\n",
        "</table>\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmrjqAELtiYw"
      },
      "source": [
        "x_train : 총 60000개의 28×28 크기의 이미지\r\n",
        "\r\n",
        "y_train : x_train의 60000개에 대한 값(0~9)이 담겨있는 레이블 데이터셋\r\n",
        "\r\n",
        "x=x_test,y_test 각각 10000개의 이미지와 레이블 데이터 셋 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxKzV4xZp_14"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "mnist = tf.keras.datasets.mnist\r\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\r\n",
        "x_train = x_train / 255.0\r\n",
        "x_test = x_test / 255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L73G3xTqt8uO"
      },
      "source": [
        "간단한 신경망 모델을 만들어보았다. 총 3개의 레이어로 이루어져 있는데 1번째 레이어는 1차원 테서로 펼치는 것이고 2번째 레이어는 1번째 레이어에서 제공되는 값인 784개의 값을 입력받아 128개로 인코딩 해주는데 활성화 함수로 ReLU 함수를 사용하였습니다. 4번째 레이어에는 총 10개의 값을 출력하는데 여기서 활성화 함수는 Softmax를 사용하였습니다. Softmax를 이용한 것은 다중분류를 위한 확률값으로 해석할 수 있도록 하기 위해서입니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xy8BG1mkqB8D"
      },
      "source": [
        "model = tf.keras.models.Sequential([\r\n",
        "        tf.keras.layers.Flatten(),\r\n",
        "        tf.keras.layers.Dense(128, activation=tf.nn.relu),\r\n",
        "        tf.keras.layers.Dense(10, activation=tf.nn.softmax)\r\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWzqjvzDuMBP"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3IUdTgeutHt"
      },
      "source": [
        "정의된 모델을 학습하기 위해서 컴파일 합니다. \r\n",
        "\r\n",
        "역전파를 통한 가중치 최적화를 위한 기울기 방향에 대한 경사하강을 위한 방법으로 Adam 옵티마이저를 이용하였고 손실함수로 다중 분류의 Cross Entropy Error인 sparse_categorical_crossentropy를 이용하였습니다. 모델 평가를 위한 평가지표로는 accuracy를 지정하였습니다.\r\n",
        "\r\n",
        "Epoch은 전체 데이터셋에 대해서 한번 학습할때의 단위로 모델의 반복횟수로는 5 Epoch를 지정하였습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y11TwBnlqEj1"
      },
      "source": [
        "model.compile(optimizer='adam',\r\n",
        "                  loss='sparse_categorical_crossentropy',\r\n",
        "                  metrics=['accuracy'])\r\n",
        "history = model.fit(\r\n",
        "         x_train, \r\n",
        "         y_train, \r\n",
        "         epochs=10\r\n",
        "         )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sr5-VQEdvLxE"
      },
      "source": [
        "모델 평가 결과 0.0944의 손실값과 0.9761의 정확도를 얻었습니다.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNjz0SOpq9J4"
      },
      "source": [
        "model.evaluate(x_test,  y_test, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-JTa4yqv6Gj"
      },
      "source": [
        "# Category 3: Convolutional Neural Network with real-world image dataset\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVJ9GQlQv5Dn"
      },
      "source": [
        "import tensorflow_datasets as tfds\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "%matplotlib inline\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxV5OKoftW1G"
      },
      "source": [
        "#데이터 종류 확인\r\n",
        "tfds.list_builders()[:10] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1Lqs_DBxTDY"
      },
      "source": [
        "dataset, info = tfds.load(name='horses_or_humans', split=tfds.Split.TRAIN, with_info=True)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPaMVkfUqRJa"
      },
      "source": [
        "데이터셋의 크기를 확인한뒤에 steps_per_epoch 지정해 줄 수 있습니다.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywF9n_TQqGNz"
      },
      "source": [
        "info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-E0zme8Fqzvj"
      },
      "source": [
        "tfds.load를 이용해서 데이터를 쉽게 불러오고 확인해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlQf80xqqa2p"
      },
      "source": [
        "train_dataset = tfds.load(name=\"horses_or_humans\", split=tfds.Split.TRAIN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwPXSMF4qk6s"
      },
      "source": [
        "for data in train_dataset.take(1):\r\n",
        "    image, label = data['image'], data['label']\r\n",
        "    plt.imshow(image.numpy()[:, :, 0].astype(np.float32), cmap=plt.get_cmap(\"gray\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xjc02xGqsHT"
      },
      "source": [
        "test_dataset = tfds.load(\"horses_or_humans\", split=tfds.Split.TEST)\r\n",
        "for data in test_dataset.take(1):\r\n",
        "    image, label = data['image'], data['label']\r\n",
        "    plt.imshow(image.numpy()[:, :, 0].astype(np.float32), cmap=plt.get_cmap(\"gray\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaRY1zzk5Cka"
      },
      "source": [
        "url을 이용해서 데이터를 불러 올 수도 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMLUrKQBq919"
      },
      "source": [
        "import urllib\r\n",
        "import zipfile\r\n",
        "url_train = \"https://storage.googleapis.com/download.tensorflow.org/data/horse-or-human.zip\"\r\n",
        "url_test = \"https://storage.googleapis.com/download.tensorflow.org/data/validation-horse-or-human.zip\"\r\n",
        "\r\n",
        "urllib.request.urlretrieve(url_train, 'horse-or-human.zip')\r\n",
        "local_zip = 'horse-or-human.zip'\r\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\r\n",
        "zip_ref.extractall('tmp/horse-or-human/')\r\n",
        "zip_ref.close()\r\n",
        "\r\n",
        "urllib.request.urlretrieve(url_test, 'testdata.zip')\r\n",
        "local_zip = 'testdata.zip'\r\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\r\n",
        "zip_ref.extractall('tmp/testdata/')\r\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqwPqPNm7rVv"
      },
      "source": [
        "ImageGenerator클래스를 사용해서 0~255의 픽셀값들을 0,1 사이로 조정한 다음 모든 이미즈의 크기를 300 x 300으로 바꿔줍니다. 그리도 다중분류의 경우 categorical또는 sparse를 사용하고 이진분류에서는 binary를 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8CBshCF6KM_"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n",
        "\r\n",
        "train_datagenerator = ImageDataGenerator(rescale = 1/255)\r\n",
        "validation_datagenerator = ImageDataGenerator(rescale = 1/255)\r\n",
        "\r\n",
        "train_generator = train_datagenerator.flow_from_directory('tmp/horse-or-human/',\r\n",
        "                                                    batch_size = 128,\r\n",
        "                                                    class_mode = 'binary',\r\n",
        "                                                    target_size = (300, 300))\r\n",
        "validation_generator = validation_datagenerator.flow_from_directory('tmp/testdata/',\r\n",
        "                                                              batch_size=32,\r\n",
        "                                                              class_mode='binary',\r\n",
        "                                                              target_size=(300, 300))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byILI22b8apX"
      },
      "source": [
        "<table>\r\n",
        "  <tr><td>\r\n",
        "    <img src=\"https://devblogs.nvidia.com/wp-content/uploads/2015/11/fig1.png\"\r\n",
        "         alt=\"Fashion MNIST sprite\" width=\"600\">\r\n",
        "  </td></tr>\r\n",
        "  <tr><td align=\"center\">\r\n",
        "    <b>Figure 1.</b> CNN Filter</a> <br/>&nbsp;\r\n",
        "  </td></tr>\r\n",
        "</table>\r\n",
        "\r\n",
        "cnn-activation-pooling 과정을 통해서 이미지 부분의 주요한 Feature 들을 추출해 냅니다. cnn을 통해서 1개의 이미지를 필터를 거친 다수의 이미지를 출력합니다.Conv2D, MaxPooling 조합으로 층을 쌓고 Classification을 위한 softmax와 마지막 출력층의 갯수는 클래스의 갯수와 동일하게 맞춰줍니다.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6IH7qn56q-2"
      },
      "source": [
        "model = tf.keras.models.Sequential([\r\n",
        "                                    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(300, 300, 3)),\r\n",
        "                                    tf.keras.layers.MaxPooling2D(2, 2),\r\n",
        "                                    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\r\n",
        "                                    tf.keras.layers.MaxPooling2D(2, 2),\r\n",
        "                                    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\r\n",
        "                                    tf.keras.layers.MaxPooling2D(2, 2),\r\n",
        "                                    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\r\n",
        "                                    tf.keras.layers.MaxPooling2D(2, 2),\r\n",
        "                                    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\r\n",
        "                                    tf.keras.layers.MaxPooling2D(2, 2),\r\n",
        "                                    tf.keras.layers.Flatten(),\r\n",
        "                                    tf.keras.layers.Dense(512, activation='relu'),\r\n",
        "                                    tf.keras.layers.Dense(1, activation='sigmoid')])\r\n",
        "\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIF7en0F_P7N"
      },
      "source": [
        "모델을 컴파일 할때 optimizer는 가장 최적화가 잘되는 알고리즘인 adam이 있지만 RMSprop를 이용하겠습니다\r\n",
        "\r\n",
        "RMSProp을 사용할 때는 학습률을 제외한 모든 인자의 기본값을 사용하는 것이 권장되며  일반적으로 순환 신경망(Recurrent Neural Networks)의 옵티마이저로 많이 사용됩니다.\r\n",
        "\r\n",
        "loss의 경우 출력층 activation이 sigmoid인 경우 binary_crossentropy를 사용하고 \r\n",
        "\r\n",
        "출력층 activation이 softmax인 경우\r\n",
        "\r\n",
        "원핫인코딩이 되어있을 경우 categorical_crossentropy를 \r\n",
        "\r\n",
        "원핫인코딩이 되어있지 않을 경우 sparse_categorical_crossentropy를 사용합니다\r\n",
        "\r\n",
        "여기서 ImageDataGenerator는 자동으로 라벨을 원핫인코딩 해줍니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y72aAtpU_Nhk"
      },
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\r\n",
        "model.compile(loss='binary_crossentropy',\r\n",
        "              optimizer=RMSprop(lr=0.001),\r\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppFqftio_bng"
      },
      "source": [
        "모델을 학습시키기전에 callback checkpoint를 지정해주겠습니다.\r\n",
        "체크포인트를 생성하는것은 val_loss 기준으로 epoch마다 최적의 모델을 저장하기 위해서 이고 \r\n",
        "\r\n",
        "checkpoint_path는 모델이 저장된 파일 명을 설정하는것입니다. 체크포인트를 만든 후에 학습을 진행하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNtyi5lZ_UZF"
      },
      "source": [
        "checkpoint_path = 'best_performed_model.ckpt'\r\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\r\n",
        "                                                save_weights_only=True,\r\n",
        "                                                save_best_only=True,\r\n",
        "                                                monitor='val_loss',\r\n",
        "                                                verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ax46jNXg_gdW"
      },
      "source": [
        "histroy = model.fit(train_generator,\r\n",
        "          steps_per_epoch = 8,\r\n",
        "          epochs = 15,\r\n",
        "          verbose = 1,\r\n",
        "          callbacks=[checkpoint],\r\n",
        "          validation_data = validation_generator,\r\n",
        "          validation_steps = 8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuIvO5KFB1y5"
      },
      "source": [
        "학습이 완료된 후에는 load_weights를 반드시 해줘야 합니다.\r\n",
        "학습 결과를 시각화 해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeCaf1LiCcCK"
      },
      "source": [
        "model.load_weights(checkpoint_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgC6SQG-_l8f"
      },
      "source": [
        "plt.figure(figsize=(12,9))\r\n",
        "plt.plot(np.arange(1,15+1), histroy.history['accuracy'])\r\n",
        "plt.plot(np.arange(1,15+1), histroy.history['loss'])\r\n",
        "plt.title('accuracy / loss')\r\n",
        "plt.xlabel('epochs')\r\n",
        "plt.ylabel('acc / loss')\r\n",
        "plt.legend(['accuracy','loss'])\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFrNYqxvEIMe"
      },
      "source": [
        "plt.figure(figsize=(8, 8))\r\n",
        "plt.subplot(2, 1, 1)\r\n",
        "plt.plot(histroy.history['accuracy'], label='Training Accuracy')\r\n",
        "plt.plot(histroy.history['val_accuracy'], label='Validation Accuracy')\r\n",
        "plt.ylim([0.8, 1])\r\n",
        "plt.legend(loc='lower right')\r\n",
        "plt.title('Training and Validation Accuracy')\r\n",
        "\r\n",
        "plt.subplot(2, 1, 2)\r\n",
        "plt.plot(histroy.history['loss'], label='Training Loss')\r\n",
        "plt.plot(histroy.history['val_loss'], label='Validation Loss')\r\n",
        "plt.ylim([0, 1.0])\r\n",
        "plt.legend(loc='upper right')\r\n",
        "plt.title('Training and Validation Loss')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.show()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6PTsuDjlzCx"
      },
      "source": [
        "# Category 4: NLP Text Classification with real-world text dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4nGPdWJmSfy"
      },
      "source": [
        "필요한 라이브러리들을 import한 다음에 json.load()를 이용하여 sarcasm 데이터를 불러옵니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1HuNpVyFUHX"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "import urllib\r\n",
        "import json\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "%matplotlib inline\r\n",
        "\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\r\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgTmNbb3mC4U"
      },
      "source": [
        "url = 'https://storage.googleapis.com/download.tensorflow.org/data/sarcasm.json'\r\n",
        "urllib.request.urlretrieve(url, 'sarcasm.json')\r\n",
        "\r\n",
        "with open('sarcasm.json', 'r') as d:\r\n",
        "    datas = json.load(d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvsksDJDmiWA"
      },
      "source": [
        "article_link : 신문기사의 링크\r\n",
        "\r\n",
        "headline : 신문기사의 헤드라인\r\n",
        "\r\n",
        "is_sarcastic : 풍자(비꼬는) 여부 (풍자면 1 아니면 0)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiOqD2YdmO96"
      },
      "source": [
        "datas[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vageRD9bnPFk"
      },
      "source": [
        "데이터 셋 구성을하기위해 피처는 sentences로 라벨은 labels로 빈 리스트를 생성하고 데이터셋을 구성해줍니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RY1yaNqnmbj4"
      },
      "source": [
        "sentences = []\r\n",
        "labels = []\r\n",
        "\r\n",
        "for data in datas:\r\n",
        "    sentences.append(data['headline'])\r\n",
        "    labels.append(data['is_sarcastic'])\r\n",
        "\r\n",
        "print(sentences[:5])\r\n",
        "print(labels[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BN6_raESnkLe"
      },
      "source": [
        "train과 validation set을 분리하기 위해서 20000개를 기준으로 데이터셋을 분리합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1bW3AYMnM77"
      },
      "source": [
        "training_size = 20000\r\n",
        "\r\n",
        "train_sentences = sentences[:training_size]\r\n",
        "train_labels = labels[:training_size]\r\n",
        "\r\n",
        "valid_sentences = sentences[training_size:]\r\n",
        "valid_labels = labels[training_size:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gImjqKHtoJm1"
      },
      "source": [
        "전처리 과정으로 Tokenizer 정의를 합니다. 단어의 토큰화를 진행합니다.\r\n",
        "\r\n",
        "num_words : 단어의 max 사이즈를 지정합니다. 가장 빈도수가 높은 단어부터 저장합니다. \r\n",
        "\r\n",
        "oov_token : 단어 토큰에 없는 단어를 어떻게 표기할 것인지 지정해줍니다.\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRDEFYaxodI6"
      },
      "source": [
        "vocab_size = 1000\r\n",
        "oov_tok = \"<OOV>\"\r\n",
        "\r\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Cwwj8UeooED"
      },
      "source": [
        "Tokenizer로 학습시킬 문장에 대한 토큰화를 진행합니다.\r\n",
        "\r\n",
        "fit_on_texts로 학습할 문장에 대하여 토큰화를 진행합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u75olzOOonsm"
      },
      "source": [
        "tokenizer.fit_on_texts(train_sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaAz8UgApIFo"
      },
      "source": [
        "문장을 토큰으로 변경합니다.\r\n",
        "\r\n",
        "texts_to_sequences: 문장을 숫자로 변환합니다. Train과 valid 데이터셋 모두 적용시켜줍니다.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yw93sjhdo_zP"
      },
      "source": [
        "train_sequences = tokenizer.texts_to_sequences(train_sentences)\r\n",
        "valid_sequences = tokenizer.texts_to_sequences(valid_sentences)\r\n",
        "\r\n",
        "print(train_sequences[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XqTTJPfpk06"
      },
      "source": [
        "시퀀스의 길이를 맞춰줍니다.\r\n",
        "\r\n",
        "maxlen : 최대 문장 길이를 정의합니다. 문장길이보다 길면 잘라냅니다.\r\n",
        "\r\n",
        "truncation : 문장의 길이가 maxlen보다 길 때 앞을 자를것인지 뒤를 자를것인지 정해줍니다.\r\n",
        "\r\n",
        "padding : 문장의 길이가 maxlen보다 짤을 때 채워줄 값을 앞을 채울지 뒤를 채울지 정의합니다.(pre - 앞부분을 채워줄때 사용)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxiW7vXSpbWb"
      },
      "source": [
        "max_length = 120\r\n",
        "trunc_type='post'\r\n",
        "padding_type='post'\r\n",
        "\r\n",
        "train_padded = pad_sequences(train_sequences, truncating=trunc_type, padding=padding_type, maxlen=max_length)\r\n",
        "valid_padded = pad_sequences(valid_sequences, truncating=trunc_type, padding=padding_type, maxlen=max_length)\r\n",
        "train_padded.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfja88yBqNJj"
      },
      "source": [
        "label 값을 numpy array로 변환해줍니다. model이 list type을 받아들이지 못하기 때문에 numpy array로 변환해줍니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fbNb80lqK9I"
      },
      "source": [
        "train_labels = np.asarray(train_labels)\r\n",
        "valid_labels = np.asarray(valid_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoX2lF7iqXws"
      },
      "source": [
        "Embedding 레이어 : 고차원을 저차원으로 축소시켜주는 역할\r\n",
        "\r\n",
        "원핫인코딩을 진행햇을때 1000차원으로 표현되는 단어들을 16차원으로 줄여줘서 sparsity문제를 해소해줍니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iy0d8sEKqW27"
      },
      "source": [
        "embedding_dim = 16\r\n",
        "\r\n",
        "#변환 전\r\n",
        "sample = np.array(train_padded[0])\r\n",
        "print(sample)\r\n",
        "\r\n",
        "#변환 후\r\n",
        "x = Embedding(vocab_size, embedding_dim, input_length=max_length)\r\n",
        "print(x(sample)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omrQeBYJskNt"
      },
      "source": [
        "Sequential 모델을 만들어주고 확인해봅니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fC-aJVAsU-I"
      },
      "source": [
        "model = tf.keras.Sequential([\r\n",
        "                             tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\r\n",
        "                             tf.keras.layers.Conv1D(128, 5, activation='relu'),\r\n",
        "                             tf.keras.layers.GlobalMaxPooling1D(),\r\n",
        "                             tf.keras.layers.Dense(24, activation='relu'),\r\n",
        "                             tf.keras.layers.Dense(1, activation='sigmoid')])\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0cLeKAGsppH"
      },
      "source": [
        "optimizer는 최적화가 잘 이루어지는 알고리즘인 adam 옵티마이저를\r\n",
        "\r\n",
        "loss는 이진 분류이기 때문에 binary_crossentropy를 사용해서 컴파일 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lx0X-WnqsoVc"
      },
      "source": [
        "model.compile(loss='binary_crossentropy', \r\n",
        "              optimizer='adam', \r\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDS7vJvbs6ld"
      },
      "source": [
        "학습을 진행하기 전에 ModelCheckPoint를 생성해줍니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRjoyWohs5qk"
      },
      "source": [
        "checkpoint_path = 'best_performed_model.ckpt'\r\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\r\n",
        "                                                save_weights_only=True,\r\n",
        "                                                save_best_only=True,\r\n",
        "                                                monitor='val_loss',\r\n",
        "                                                verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SirheKJ3tCms"
      },
      "source": [
        "history = model.fit(train_padded,\r\n",
        "          train_labels,\r\n",
        "          epochs=50,\r\n",
        "          callbacks = [checkpoint],\r\n",
        "          validation_data=(valid_padded, valid_labels),\r\n",
        "          verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11HtgNRHtYiN"
      },
      "source": [
        "학습이 완료된 이후에는 load_weights를 반드시 해줍니다.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUNryAj9vp7e"
      },
      "source": [
        "model.load_weights(checkpoint_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37hYdys9tSJx"
      },
      "source": [
        "plt.figure(figsize=(12, 9))\r\n",
        "plt.subplot(2, 1, 1)\r\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\r\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\r\n",
        "plt.legend(loc='lower right')\r\n",
        "plt.title('Training and Validation Accuracy')\r\n",
        "\r\n",
        "plt.subplot(2, 1, 2)\r\n",
        "plt.plot(history.history['loss'], label='Training Loss')\r\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\r\n",
        "plt.legend(loc='upper right')\r\n",
        "plt.title('Training and Validation Loss')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXCsKFGcqpti"
      },
      "source": [
        "# Category 5: Sequence Model with real-world numeric dataset\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuPSZ9F6rPjQ"
      },
      "source": [
        "필요한 라이브러리들을 import합니다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFMk5Wetto39"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZloRTN7vrMOm"
      },
      "source": [
        "def plot_series(time, series, format=\"-\", start=0, end=None):\r\n",
        "    plt.plot(time[start:end], series[start:end], format)\r\n",
        "    plt.xlabel(\"Time\")\r\n",
        "    plt.ylabel(\"Value\")\r\n",
        "    plt.grid(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7R7VSbWrNLQ"
      },
      "source": [
        "!wget --no-check-certificate \\\r\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/Sunspots.csv \\\r\n",
        "    -O /tmp/sunspots.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGu81yGjrehl"
      },
      "source": [
        "sunspots 데이터를 불러온 다음에 데이터를 확인해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHuJRQTbrOUz"
      },
      "source": [
        "time_step = []\r\n",
        "sunspots = []\r\n",
        "\r\n",
        "with open('/tmp/sunspots.csv') as csvfile:\r\n",
        "  reader = csv.reader(csvfile, delimiter=',')\r\n",
        "  next(reader)\r\n",
        "  for row in reader:\r\n",
        "    sunspots.append(float(row[2]))\r\n",
        "    time_step.append(int(row[0]))\r\n",
        "\r\n",
        "series = np.array(sunspots)\r\n",
        "time = np.array(time_step)\r\n",
        "plt.figure(figsize=(10, 6))\r\n",
        "plot_series(time, series)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfiYVGOvrmzc"
      },
      "source": [
        "trian과 valid 데이터 셋을 만들어 줍니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XS8B8Z2frdVB"
      },
      "source": [
        "split_time = 3000\r\n",
        "time_train = time[:split_time]\r\n",
        "x_train = series[:split_time]\r\n",
        "time_valid = time[split_time:]\r\n",
        "x_valid = series[split_time:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XEghiawrt0v"
      },
      "source": [
        "window_size = 30\r\n",
        "batch_size = 32\r\n",
        "shuffle_buffer_size = 1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByoezNN8r50O"
      },
      "source": [
        "windowed_dataset은 Time Series 데이터셋을 생성할때 매우 유용합니다.\r\n",
        "\r\n",
        "window : 그룹화 할 윈도우의 크기(갯수)\r\n",
        "\r\n",
        "drop_remainder : 남은 부분을 버릴지 살릴지 \r\n",
        "\r\n",
        "shift : 1 iteration 당 몇 개씩 이동할 것인지\r\n",
        "\r\n",
        "flat_map : 데이터셋에 함수를 apply 해주고 결과를 flatten하게 펼쳐 줍니다.\r\n",
        "\r\n",
        "shuffle : 데이터 셋을 섞어 줍니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9f4pVz-ruOs"
      },
      "source": [
        "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\r\n",
        "    series = tf.expand_dims(series, axis=-1)\r\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\r\n",
        "    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\r\n",
        "    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\r\n",
        "    ds = ds.shuffle(shuffle_buffer)\r\n",
        "    ds = ds.map(lambda w: (w[:-1], w[1:]))\r\n",
        "    return ds.batch(batch_size).prefetch(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7i8tMsA_tLp9"
      },
      "source": [
        "window_dataset을 이용해서 time series 데이터셋을 생성해줍니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBEZ_brZrz_b"
      },
      "source": [
        "train_set = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\r\n",
        "print(train_set)\r\n",
        "print(x_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4TgDiVOtRDM"
      },
      "source": [
        "LSTM 모델을 생성해줍니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MU_S1g7dtIev"
      },
      "source": [
        "model = tf.keras.models.Sequential([\r\n",
        "                                    tf.keras.layers.Conv1D(filters=32, kernel_size=5,strides=1, padding=\"causal\",activation=\"relu\",input_shape=[None, 1]),\r\n",
        "                                    tf.keras.layers.LSTM(64, return_sequences=True),\r\n",
        "                                    tf.keras.layers.LSTM(64, return_sequences=True),\r\n",
        "                                    tf.keras.layers.Dense(30, activation=\"relu\"),\r\n",
        "                                    tf.keras.layers.Dense(10, activation=\"relu\"),                       \r\n",
        "                                    tf.keras.layers.Dense(1)])\r\n",
        "\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzgZ17y5tvqD"
      },
      "source": [
        "LearningRateScheduler : 모델 학습동안에 일반적으로 행하는 것은 epochs에 따라 learning rate를 decay시켜주는 것입니다.\r\n",
        "\r\n",
        "옵티마이저는 Stochastic Gradient Descent로 확률적 경사 하강법을 사용하였습니다.한번 학습할 때 모든 데이터에 대해 가중치를 조절하는 것이 아니라, 램덤하게 추출한 일부 데이터에 대해 가중치를 조절합니다.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94KfotLhts_f"
      },
      "source": [
        "lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-8 * 10 ** (epoch / 20))\r\n",
        "optimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)\r\n",
        "model.compile(loss=tf.keras.losses.Huber(),\r\n",
        "              optimizer=optimizer,\r\n",
        "              metrics=[\"mae\"])\r\n",
        "\r\n",
        "history = model.fit(train_set, epochs=100, callbacks=[lr_schedule])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xp19Z5F1w8Bg"
      },
      "source": [
        "학습이 끝난뒤에 예측을 해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfWelFc4tyVt"
      },
      "source": [
        "def model_forecast(model, series, window_size):\r\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\r\n",
        "    ds = ds.window(window_size, shift=1, drop_remainder=True)\r\n",
        "    ds = ds.flat_map(lambda w: w.batch(window_size))\r\n",
        "    ds = ds.batch(32).prefetch(1)\r\n",
        "    forecast = model.predict(ds)\r\n",
        "    return forecast\r\n",
        "\r\n",
        "rnn_forecast = model_forecast(model, series[..., np.newaxis], window_size)\r\n",
        "rnn_forecast = rnn_forecast[split_time - window_size:-1, -1, 0]\r\n",
        "\r\n",
        "plt.figure(figsize=(10, 6))\r\n",
        "plot_series(time_valid, x_valid)\r\n",
        "plot_series(time_valid, rnn_forecast)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkwOzTR7wjia"
      },
      "source": [
        "tf.keras.metrics.mean_absolute_error(x_valid, rnn_forecast).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNbzhcpKw33B"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}